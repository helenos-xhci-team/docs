\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

\section{Try it yourself}

This section provides a brief crashcourse on how to get our work running.

After cloning our repository, HelenOS should be compiled as usual according to
the official guide. Namely, the following two commands should suffice given
that necessary build dependencies are installed:

\begin{bdsh}
$ ./tools/toolchain.sh amd64
$ make
\end{bdsh}

The \texttt{make} command prompts with a config window. Select \textit{amd64}
default profile and turn off SMP because SMP support is currently broken in
HelenOS \footnote{\href{http://www.helenos.org/ticket/461}{http://www.helenos.org/ticket/461}}
\footnote{\href{http://www.helenos.org/ticket/387}{http://www.helenos.org/ticket/387}}
\footnote{\href{http://www.helenos.org/ticket/388}{http://www.helenos.org/ticket/388}}. Then simply submit it.

\subsection{Running in QEMU}

A reasonably recent (version 2.10 or newer) QEMU is required.

The script \texttt{ew.py}, which is the official way to launch QEMU with
correct parameters, has been modified to include both an xHCI controller and a
USB tablet device (as introduced in \ref{sec:usb-tablet-driver}) by default.
Therefore, one should just run:

\begin{bdsh}
$ ./tools/ew.py
\end{bdsh}

After the system starts, the tablet device should function correctly. The log
of the HelenOS xHCI driver can be viewed with

\begin{bdsh}
$ edit /log/xhci
\end{bdsh}

Other related logfiles are \texttt{/log/usbhub}, \texttt{/log/usbhid} and
\texttt{/log/usbmid}.

\subsection{Testing with tmon}

\texttt{usb-tmon} (described in \ref{sec:tmon}) is a custom USB device emulated
by QEMU. \texttt{tmon} allows us to read and write data to and from endpoints
types and provides some diagnostics.

To use \texttt{usb-tmon}, first compile our version of QEMU that contains the
\texttt{usb-tmon} driver:

\begin{bdsh}
$ cd qemu
$ mkdir build; cd build
$ ../configure --target-list=x86_64-softmmu
$ make
\end{bdsh}

Then start HelenOS in your freshly-compiled QEMU:

\begin{bdsh}
$ ./tools/ew.py -qemu_path ../qemu/build/x86_64-softmmu/
\end{bdsh}

Then switch to the QEMU monitor (Ctrl+Alt+2) and enter \texttt{device\_add
usb-tmon}. Then switch back to HelenOS (Ctrl+Alt+1) and enter \texttt{tmon
list} to the console. A new device should appear. Now you can start the test,
for example with:

\begin{bdsh}
$ tmon test-bulk-in
\end{bdsh}

\subsection{Running on real hardware}

Our work has been tested on a desktop with Teratrend $\mu$DP720202 Rev. 1.0
PCIe card and on a ThinkPad x240 laptop with Intel 8 series xHC (PCI ID
8086:9C31).  Regarding peripherals, several no-name and branded USB2 and USB3
hubs, mice, keyboards, multi-interface keyboards (with multimedia keys) and
flash drives have been tested.

It is possible to boot \texttt{image.iso} on the target computer, however
booting over the network proved to be much more convenient during development.
For netboot, we used GRUB instead of the more widespread pxelinux, as HelenOS
uses \texttt{multiboot} protocol and pxelinux does not support it.

The following commands can be used to generate a PXE GRUB image:

\begin{bdsh}
$ grub-mkimage --format=i386-pc --output=core.img --prefix="(pxe)" pxe tftp
$ cat /usr/lib/grub/i386-pc/pxeboot.img core.img > grub2pxe
\end{bdsh}

Then \texttt{/usr/lib/grub/i386-pc}, all files from
\texttt{helenos/boot/distroot/boot/} and the following \texttt{grub.cfg} are
placed to the TFTP server root and \texttt{grub2pxe} is booted.

\begin{bdsh}
set timeout=1
insmod multiboot
menuentry "helenos" {
  set root=(pxe)
  multiboot /kernel.bin
  module    /ns /boot/ns
  module    /loader /boot/loader
  module    /init /boot/init
  module    /locsrv /boot/locsrv
  module    /rd /boot/rd
  module    /vfs /boot/vfs
  module    /logger /boot/logger
  module    /ext4fs /boot/ext4fs
  module    /initrd.img /boot/initrd.img
}
\end{bdsh}

\section{Future Development}

This chapter outlines features, which were considered optional extensions of
the project but were not realized due to time, complexity or other constraints.
These features provide good starting points for the future development of the
USB stack.

\subsection{Power Management}

The xHCI Specification focuses a lot on power management. Devices have multiple
states corresponding to the amount of power used, and links between devices
do so as well. Of course, to fully utilize these features, there must have been
a system-wide mechanism to declare intentions about power management. We are not
there yet.

Despite that, there are a few power switches we could toggle just now. Given the
small number of drivers implemented, we could for example make the USB fallback
driver suspend the device.

We consider power management a topic that needs to be addressed on the
system level, and as such we did not pay much attention to it.

\subsection{Asynchronous I/O}

The aspect we care about though is performance. Although the primary goal of
this project was to allow users to use USB on machines equipped only with xHC,
the performance benefit of USB 3 is not negligible, and could be well worth it.

Every transfer type of USB focuses on different characteristics of the
transfer. Interrupt pipes strive for the lowest latency possible -- in this
field, the synchronous semantics works very well. The latency between receiving
interrupts and delivering them as fast as an IPC reply to a call.

Considering isochronous endpoints, it is simply not possible to satisfy them
with a synchronous API. For that reason, we decided to change the semantics to asynchronous
one. The downside of the current approach is that the HC driver is forced to
copy the data out from the shared buffer, because its ownership is temporary.
But since isochronous transfers in xHCI are just a proof-of-concept and
there are no drivers for real devices yet, we have decided not to optimize
prematurely.

The bulk transfer type can however be utilized both synchronously and
asynchronously, and both use cases have their advantages as well as disadvantages. While synchronous API is easier to
work with, asynchronous would offer much better performance though. However, as there is just
a single driver using bulk endpoints, we have decided not to complicate matters and stay
with synchronous-only API for bulk pipes.

\subsection{Isochronous Transfers in UHCI+OHCI+EHCI}

The isochronous module implemented in xHCI can be thought of as a generic scheduling
framework for isochronous transfers. The scheduler behaves like a leaky bucket,
scheduling the transfers to xHC at a constant rate while throttling the device
driver. This is the most complicated part of handling isochronous transfers,
and implementing an asynchronous API for drivers is a good opportunity to
generify it for other drivers as well.

Implementing the support for isochronous transfers for older HCs is a bit
harder task, though. Those drivers use proprietary data structures for isochronous
transfers, and require the software to schedule transfers so all time
constraints are satisfied. (The xHC does it in hardware, and software is just
responsible for issuing transfers fast enough.) Also, because of a heavily
simplified implementation of periodic scheduling in the former HC drivers,
substantial refactoring is inevitable. That however requires a deeper
understanding of inner workings of individual HCIs.
