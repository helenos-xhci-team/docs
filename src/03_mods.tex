\chapter{USB Subsystem Modifications}
\label{usb-refactoring}

Before we start explaining individual modifications to the USB stack, we think
it is good to explain the overall architecture of it. Please see the figure
\ref{fig:stack-architecture}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{stack-architecture}
	\caption{The runtime architecture of the USB stack, showing the communication paths}
	\label{fig:stack-architecture}
\end{figure}

At the highest logical level, the USB device driver communicates with the
device using USB packets. The packets are sent through pipes, illusion of which
is created by the \lib{libusbdev} library. The library communicates with the HC
driver using the \texttt{usbhc} driver interface. The drivers actually
communicate using the Device Driver Framework, which in the end uses the
synchronous interface built upon asynchronous IPC mechanisms in kernel. The
scope of USB stack is limited to the USB device driver tasks, and to the USB
Host Controller driver tasks. Because the USB Device Framework hasn't changed much
in USB 3, our primary focus related to implementing USB 3 support was the HC
driver. The xHC driver itself was exhaustively explained in the previous
chapter, this chapter explains the modifications needed to implement it.
Furthermore, it contains the modifications needed or supporting the
implementation of other key requirements of the project.

\input{03_bus}
\input{03_disconnect}
\input{03_polling}

\section{A Library Module for USB Hubs}
\label{hub-port-refactoring}

We introduced a new module to support writing hub drivers:
\file{uspace/lib/usb/include/usb/port.h}{usb/port.h}. It solves the problem of
hub drivers, that events are announced through a single channel, even though
they need to wait for each other. The implementation of this module was
motivated not only by the need of refactoring, nor because we wanted to share
the functionality with the xHCI driver, but because the previous implementation
of \lib{usbhub} driver synchronized the fibrils wrong. There might have been
situations in which two fibrils were spawned and enumerated the same device.

To solve the issue a state information is managed for every port, represented
by the structure \struct{usb_port_t}. It contains the current port state, one
of the following:

\begin{description}
\begingroup \leftskip=1cm \rightskip=\leftskip
\setcounter{enumi}{-1}
	\item[\state{Disabled}]
		There has been no activity on the port yet, or it is already over.
		Initial state.

	\item[\state{Connecting}]
		A connected event came, the enumerating fibril was started. It hasn't
		finished yet, so the device is not enumerated yet. Also, the device is
		still connected, no error event came while the connecting is in
		progress.

	\item[\state{Enumerated}]
		The device was successfully enumerated, so it has to be removed after
		it will be disconnected.

	\item[\state{Disconnecting}]
		A disconnected event came after the device was enumerated, so
		a removing fibril was started.

	\item[\state{Error}]
		An event about the device disconnection state came while the
		enumeration was in progress, so the enumeration fibril has to be
		stopped. This is achieved by returning \macro{EINTR} from the waiting
		methods.

\endgroup
\end{description}

The basic synchronization invariant is that the state never changes unless the
guard is locked. It is expected that the guard is not held for a long time, so
the fibril generating events shall not be blocked indefinitely. The only
exception being the final phase of enumeration (announcing the device to the
bus), which once started, cannot be easily interrupted -- but this operation on
the bus is also expected to run for a limited time only.

The scheme of states and allowed transitions can be seen on figure
\ref{fig:port-states}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{port-states}
	\caption{A graph of port states and their transitions.}
	\label{fig:port-states}
\end{figure}

The transitions are triggered by delivering events to the module. This is done
by calling following functions:

\begin{itemize}
	\item \fnc{usb_port_connected}
	\item \fnc{usb_port_disabled}
	\item \fnc{usb_port_enabled}
\end{itemize}

The \fnc{connected} and \fnc{disabled} functions take a callback as an
argument. If the respective state transition is triggered, this callback is run
in a separate fibril. These functions can block to obtain the port guard, and
the \fnc{disabled} event can furthermore block while waiting for the
enumeration fibril to terminate. But before it does, it makes sure the worker
will be notified as soon as possible.

To make it work, the callbacks must not block while holding the guard. They
can, however, wait for the enabled event, signalling the completion of port
reset, using a function \fnc{usb_port_wait_for_enabled}. The caller is required
to check the return value of it -- it can either finish with \macro{EOK},
timeout with \macro{ETIMEOUT} or be interrupted with \macro{EINTR}. The enabled
event is sent by calling the function \fnc{usb_port_enabled}.

When this fibril management is separated, an implementation of USB hub driver or
root hub driver is very simple. The hub driver just initializes the
\struct{usb_port_t} structure for every port it manages, then waits for events
from the hardware (either by poling the \textit{Status Change Endpoint}, or by
waiting for an interrupt), and forwards the events to this module, providing an
implementation of the enumeration and removal process.

One last thing to note is what to do when the structure is finalized, but
a device is still connected. It might happen that there is a subtree of devices
and hubs being removed because the subtree root $R$ has been unexpectedly
removed. In that case, a port disconnect is delivered to the parenting hub,
which triggers the device removal process. The hub $R$ then receives the DDF
signal \fnc{device_offline}, and starts removal if his own children. But
there's a catch: USB hierarchy is presented as a flat one to the DDF framework.
That means that the devman has no clue about hubs (it sees them as ordinary USB
devices), and considers that all of them are connected to the HC directly.
Device removal is prepared for the situation that device will remove also its
\textit{children} devices, but serializes removal of \textit{sibling} devices.
Therefore, recursive removal of hubs creates a deadlock in devman.

Fortunately, the HC driver must be prepared for badly written drivers, so it
cleans up after the device function is unbound. This cleanup includes also
removal of its former children devices -- so the workaround for this problem is
simply leaving those devices connected, as the HC will remove them itself. This
state transition is denoted by dotted line in the graph, and is triggered by
finalizing the port structure while a device is enumerated.

\section{USB Tablet Driver}

This modification is very standalone and seemingly simple, yet very useful and
appreciated. We extended the HID driver to support absolutely positioned
devices. That means, one can now connect a USB tablet and it will work in
HelenOS. If you are still wondering what this could be useful for (people using
USB tablets are usually graphic designers or photographers, not microkernel
developers), try running QEMU with an emulated one:

\begin{bdsh}
$ qemu-system-x86_64 -enable-kvm -usb -device usb-tablet -boot d -cdrom image.iso
\end{bdsh}

When using mouse with relative positioning (PS/2, USB mouse), one has to first
click inside the window of QEMU to let it grab input. To release it again,
a special key combination (for current QEMU Ctrl+Alt+G) must be pressed. When
using an emulated USB tablet instead, the mouse is not ``locked'' inside the
window, but it can freely move in and out and still be registered by the guest
OS.

\section{DMA buffers}
\label{sec:dma_bufs}

A simple but repeated scenario gave rise to another new submodule of
\lib{libusbhost}. It started as a thin abstraction, and its purpose grew to
a major performance optimization targeting the whole system. Therefore, we plan
to discuss with HelenOS core developers to incorporate it in other drivers as
well.

\subsection{The original purpose}

A common task for drivers is allocating memory for buffers, that are accessible
for DMA. Often there are some restrictions given by the hardware driven -- the
buffer must be placed in the lower 32bit addressable space, it has to be
aligned, physically contiguous, or possibly not crossing a page boundary.

Even if the buffer is intended for the hardware use only (like xHCI
scratchpads), the driver must keep the pointer to where the buffer is mapped
inside its virtual address space in order to release the memory once it is not
needed anymore. Hardware devices do not share the virtual address space with
the task though, so the driver must always obtain also the physical address of
the buffer. To satisfy all of these requirements, the HelenOS kernel offers
a specialized API. The driver just needs to call a function
\fnc{dma_map_anonymous}, and is given both the virtual and physical address. It
is able to specify its requirements using flags. This API is powerful, but
complex and inconvenient to use.

The authors of the former USB stack addressed the complexity by creating
utility functions in \header|<usb/host/utils/malloc32.h>|. These functions offer
a familiar interface of \fnc{malloc}/\fnc{free}. The \fnc{malloc32} function
intentionally discards the physical address provided by
\fnc{dma_map_anonymous}, in order to be simple to use. To retrieve the physical
address later, one can use another utility function, \fnc{addr_to_phys}. This
one is a simple wrapper for a syscall.

Even previous authors of the USB subsystem were aware of it being a syscall,
and tried to cache the physical address where it would be used unnecessarily
multiple times. The usage scheme of these functions then grew wild: the memory
was allocated by \fnc{malloc32}, just to be translated by \fnc{addr_to_phys} on
the next line. These two pointers were then stored inside the same structure.

Being aware of this use-case, we created a different submodule for allocation
of DMA buffers. Instead of a plain pointer, the caller uses a structure called
\struct{dma_buffer_t}. This structure can store both the virtual and physical
address of the buffer, accessible through fields \struct{virt} and
\struct{phys}. The buffer allocation is made simply by using a function called
\fnc{dma_buffer_alloc}, which fills the specified buffer with pointers to
a properly sized allocated space. When the buffer is no longer needed, it is
freed by calling \fnc{dma_buffer_free}.

When the buffer is contiguous in memory, it's easy to calculate the physical
address ourselves, without a help of the kernel. To obtain a physical address
for a concrete position in the buffer, one just needs to call
\fnc{dma_buffer_phys}, which translates a virtual address pointing inside
a buffer to a corresponding physical address.

One more reason to drop the previous \fnc{malloc32} helpers completely is that
developers that are beginners to HelenOS might see \fnc{malloc32} as a weird
name for an ordinary \fnc{malloc}, and start using it to allocate ordinary
memory, which is very wasteful and expensive.

Even authors of the previous implementation started to overlook fact, that
\fnc{malloc32} actually allocates a whole page, and implemented some operations
very prodigally. The most notable occurrence was in transfer handling -- every
transfer descriptor and queue head was allocated in a separate memory page.
From the performance point of view, allocation and clearing of several memory
pages just to issue one short transfer is an extreme overhead. Furthermore,
a TD can describe at most 8KB transfer, so a linear number of TDs are used.
On the other hand, it is also waste of memory to allocate the memory in
advance. So we at least merged all the needed structures to one allocated
buffer to reduce the overhead to one page only.


\subsection{Policies}

To accomplish a goal described in the following chapter, we needed a way how to
express constraints put on DMA buffers by individual drivers. After evaluating
the individual needs, it came out that there are just two of them:
a requirement of using 32-bit addresses, and a requirement of physical
contiguity. However, the contiguity is not just binary (either fully contiguous
or not). For example, xHCI-compatible controller is able to do bytewise
scatter-gather. That means it can handle also non-contiguous buffers,
translated as a scatter-gather on pages. On the other hand, every TRB can
describe a 64KB block, so it can benefit from the buffer being contiguous by
using less TRBs per transfer.

We used our benchmarking system to measure performance of issuing transfers.
The scenario contains a QEMU virtual device (tmon) receiving our data and
a software driver of this device issuing transfer. The most important aspect of
the testing is that neither of these two entities actually care about the data
being transmitted -- the driver just allocates a buffer of garbage and sends
it. The receiving side just counts the size of the received data. It is
a completely artificial scenario, but allows us to measure the overhead induced
by handling transfers, thus limiting the possible performance in real scenarios.

We measured the maximum performance of bulk transfers of a 64KB buffer. That is
the maximum size of data transferred over IPC in HelenOS. When we relied on the
buffer being contiguous, we could always issue single TRB per transfer. When
not, we had to split the transfer to 16 TRBs. In the first case, the average
performance was around 300 MB/s, in the second one, it dropped to 60 MB/s. That
shows that the overhead is significant and we should use as large TRBs as
possible.

It seems that the best solution would be to allocate chunks of 16 pages and
split the data accordingly. However, using a buffer vector would introduce yet
another API which would drivers have to use instead of a standard one. And with
the experience with IO vectors from QEMU, we decided this is not a good option.
The more when the same task can be solved by a slightly smarter kernel
allocator, mapping the chunks into contiguous virtual memory. But anyway, we
needed a way to express how big the chunks have to be.

Let us introduce another scenario, which could've worked if we had the power.
Imagine a VFS server trying to copy 16 megabytes of data from hard drive to
a flash drive. Let's say that the hard drive driver is capable of arbitrary
scatter-gather operations, but handles 32-bit addresses only. On the other
side, there is an xHC, which can handle 64-bit addresses, but it works best
with 64KB-contiguous chunks. However, in between there sits a USB mass storage
driver, which (just to demonstrate the idea) can issue only 32KB transfers,
so it splits bigger ones to multiple calls. In an ideal world, the VFS driver
would allocate a 16 MB buffer that is in the lower 4 gigabytes of memory, and
every 32KB chunk of it is physically contiguous. Then it would share the buffer
using IPC memory sharing capabilities, so that the hard drive driver loads the
data in it. Then, without any copying, the VFS server would share the buffer
all the way down to the xHC driver, which would issue 32 KB transfers as
ordered by the mass storage driver with the data directly, again without any
further copying.

This level of copy-free efficiency is thought to be reasonably possible only in
unikernel systems. But for an unknown reason we were too enthusiastic to let
it be and decided to do a little experiment, designing a system which would
allow this scenario to happen even in HelenOS.

The answer to all these issues is surprisingly simple. We use a pointer-sized
bitmap called \struct{dma_policy_t} to carry flags. The flag on bit $i$ denotes
that the buffer may be split into $2^{i+1}$B-sized chunks that are physically
contiguous. As the minimal granularity is \macro{PAGE_SIZE}, the lower bits can
be used to carry different flags. Currently, we use just one to denote the
32-bit restriction.

The policy is a part of the pipe description, which is passed from HC driver to
the USB device driver. It may than allocate a buffer according to this
policy, and be sure that the HC driver will be able to use it directly. The
policy a buffer is allocated with is passed along, for the HC driver to check.
Because we generally trust the driver, there is no further checking. We do not
trust the human writing it though so the policy is not automatically assumed
but must be passed explicitly with the buffer pointer. This has another benefit
-- in case the policy is not exactly satisfied (e.g. the xHC driver is given
a non-contiguous buffer), it may decide that instead of bouncing it, which is
expensive, it just uses a page-sized chunks for scatter-gather.

If this mechanism would be adopted by the rest of the system, the scenario
described above would start with the VFS driver fetching policies from the
lower-layer drivers. The HC driver would respond with a policy requiring 64KB
chunks, but it would be modified by the mass storage driver to loosen the
contiguity requirements, because it knows that the limit is actually 32KB. This
can be done by a simple bitwise AND operation. From the other side, the hard
drive driver would respond with a policy requiring 32-bit addresses. The VFS
driver then computes a bitwise OR, creating a policy that satisfies both sides.

\section{Memory sharing}

As promised in the previous section, we modified the interface of host
controller driver. Instead of sending large data through IPC, which requires
kernel to copy data from one task to another, we now use IPC memory sharing.
While it might be actually slower for small transfers (latency benchmarks shows
the difference is not relevant), it makes a big difference for large transfers.
Utilizing the DMA policies described in the previous section, we achieved
a copy-free path from USB device driver to the xHC.

By switching to memory sharing, we released the size limit of one transfer --
the shared memory area might be arbitrarily large, as opposed to the 64KB
artificial limit imposed by the kernel on data transfers. That introduced the
need to actually split transfers to multiple TRBs if needed, so the benchmarked
performance of previously described scenario dropped to 200 MB/s. However, by
using transfers of size 16 MB, we can reach a measured bandwidth of around 1.3~GB/s.
As the theoretical limit of the USB 3.1 bus is 671 MB/s, we can say that the
bottleneck is no longer in the xHCI driver.

As for the other drivers, the benchmarks on EHCI shown an improvement at 64KB
blocks from 20 MB/s to 25 MB/s, but by using much bigger blocks (larger than 32
MB) we were able to reach 360 MB/s. Again, this is much bigger than the
theoretical limit of 60 MB/s for the bus due to the test nature and virtual
environment.
